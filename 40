"""
Sequence-modeling baseline (LSTM & Transformer) for time-series regression.
Key points implemented
––––––––––––––––––––––––
1.  Fit `StandardScaler` **only** on the full training set, then apply to val/test.
2.  Validation set = last 10 % of the original training dataframe (time-ordered).
3.  Sliding-window dataset that keeps index for writing predictions back.
4.  LSTM with optional dropout.
5.  Transformer with *learnable* positional encoding + causal mask.
6.  Training helper with early-stopping on validation MAE.

Assumes you already have
    train_df, test_df, feature_cols  
prepared exactly like in your LightGBM script.
"""

from __future__ import annotations

import math
from pathlib import Path
from typing import List, Tuple

import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from torch.utils.data import Dataset, DataLoader

# =============== 1. split train / val ==================

def chronological_split(df, time_col: str = "date", val_frac: float = 0.1):
    """Return train_df, val_df split chronologically by *time_col*.
    The last *val_frac* fraction is taken as validation set.
    """
    df_sorted = df.sort_values(time_col)
    split = int(len(df_sorted) * (1 - val_frac))
    train_df = df_sorted.iloc[:split].copy()
    val_df = df_sorted.iloc[split:].copy()
    return train_df, val_df


# =============== 2. sliding-window Dataset ==============

# class SequenceDataset(Dataset):
#     def __init__(
#         self,
#         df,
#         feature_cols: List[str],
#         seq_len: int = 10,
#     ) -> None:
#         self.data: List[np.ndarray] = []
#         self.target: List[float] = []
#         self.row_idx: List[int] = []

#         for theme, group in df.groupby("theme"):
#             group = group.sort_values("date")
#             X = group[feature_cols].values
#             y = group["y_clip"].values
#             idx = group.index.values
#             for i in range(len(group) - seq_len):
#                 self.data.append(X[i : i + seq_len])
#                 self.target.append(y[i + seq_len])
#                 self.row_idx.append(idx[i + seq_len])

#         self.data = np.asarray(self.data, dtype=np.float32)
#         self.target = np.asarray(self.target, dtype=np.float32)
#         self.row_idx = np.asarray(self.row_idx)
#         self.seq_len = seq_len

#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, idx: int):
#         return (
#             torch.from_numpy(self.data[idx]),
#             torch.tensor(self.target[idx]),
#             int(self.row_idx[idx]),
#         )
# ------------ SequenceDataset 修订版 ----------------

class SequenceDataset(Dataset):

    """

    Sliding-window dataset.

    · 仅当 theme 的样本数 ≥ seq_len+1 时才产生窗口

    · 在 __init__ 里一次性转成 torch.Tensor，保证后续安全

    """

    def __init__(self, df, feature_cols, seq_len=10):

        self.seq_len = seq_len

        win_list, tgt_list, idx_list = [], [], []

        for theme, g in df.groupby("theme"):

            g = g.sort_values("date")

            if len(g) <= seq_len:          # 过滤过短分组

                continue

            X = g[feature_cols].to_numpy(dtype=np.float32, copy=False)

            y = g["y_clip"].to_numpy(dtype=np.float32, copy=False)

            idx = g.index.to_numpy()

            for i in range(len(g) - seq_len):

                win_list.append(X[i:i + seq_len])

                tgt_list.append(y[i + seq_len])

                idx_list.append(idx[i + seq_len])

        if not win_list:

            raise ValueError("No valid windows generated; "

                             "check seq_len or data size.")

        # ---- 直接转 3 个 torch.Tensor ----

        self.windows = torch.tensor(np.stack(win_list), dtype=torch.float32)   # (N, T, F)

        self.targets = torch.tensor(tgt_list, dtype=torch.float32)             # (N,)

        self.row_idx = torch.tensor(idx_list, dtype=torch.long)               # (N,)

    def __len__(self):

        return self.targets.size(0)

    def __getitem__(self, i):

        return self.windows[i], self.targets[i], self.row_idx[i]

# =============== 3.  models =============================

class LSTMPredictor(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, drop: float = 0.2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=drop if num_layers > 1 else 0.0,
        )
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):  # x: (bs, T, d)
        out, _ = self.lstm(x)
        pred = self.fc(out[:, -1, :])  # last time-step
        return pred.squeeze(-1)


class PositionalEncoding(nn.Module):
    """Learnable positional encoding."""

    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        self.pe = nn.Parameter(torch.zeros(max_len, d_model))
        nn.init.uniform_(self.pe, -0.02, 0.02)

    def forward(self, x):  # x (bs, T, d)
        T = x.size(1)
        return x + self.pe[:T]


def causal_mask(size: int, device):
    """Triangular mask so each position can attend to <= itself."""
    return torch.triu(torch.ones(size, size, device=device), diagonal=1).bool()


class TransformerPredictor(nn.Module):
    def __init__(
        self,
        input_dim: int,
        d_model: int = 64,
        nhead: int = 8,
        num_layers: int = 2,
        dropout: float = 0.1,
    ):
        super().__init__()
        assert d_model % nhead == 0, "d_model must be divisible by nhead"
        self.input_proj = nn.Linear(input_dim, d_model)
        self.pos_enc = PositionalEncoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model,
            nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)
        self.fc = nn.Linear(d_model, 1)

    def forward(self, x):  # x: (bs, T, d_in)
        x = self.input_proj(x)
        x = self.pos_enc(x)
        bs, T, _ = x.shape
        mask = causal_mask(T, x.device)
        out = self.encoder(x, mask=mask)
        pred = self.fc(out[:, -1, :])
        return pred.squeeze(-1)


# =============== 4. training helper =====================

def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    num_epochs: int = 10,#100,
    lr: float = 3e-4,
    patience: int = 10,
    device: str | torch.device = "cpu",
):
    model = model.to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.L1Loss()
    best_state, best_mae, epochs_no_improve = None, float("inf"), 0

    for epoch in range(1, num_epochs + 1):
        model.train()
        for X, y, _ in train_loader:
            X, y = X.to(device), y.to(device)
            optim.zero_grad()
            loss = criterion(model(X), y)
            loss.backward()
            optim.step()

        # ---------- validation ----------
        model.eval()
        with torch.no_grad():
            y_true, y_pred = [], []
            for X, y, _ in val_loader:
                X = X.to(device)
                pred = model(X).cpu().numpy()
                y_true.append(y.numpy())
                y_pred.append(pred)
            y_true = np.concatenate(y_true)
            y_pred = np.concatenate(y_pred)
            mae = mean_absolute_error(y_true, y_pred)
        print(f"Epoch {epoch:03d} | val MAE = {mae:.6f}")

        # early stopping
        if mae + 1e-7 < best_mae:
            best_mae = mae
            best_state = {k: v.cpu() for k, v in model.state_dict().items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print("Early stopping ✓")
                break

    model.load_state_dict(best_state)
    return model, best_mae


# =============== 5. main routine (put inside if __name__…) =====

def run_pipeline(
    train_df,
    test_df,
    feature_cols: List[str],
    seq_len: int = 10,
    batch_size: int = 128,
    model_type: str = "lstm",  # or "trans"
    device: str | torch.device = "cpu",
):
    # 1.  chrono split
    tr_df, val_df = chronological_split(train_df, time_col="date", val_frac=0.1)

    # 2.  scaling (fit on *whole* tr_df)
    scaler = StandardScaler()
    tr_df[feature_cols] = scaler.fit_transform(tr_df[feature_cols])
    val_df[feature_cols] = scaler.transform(val_df[feature_cols])
    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

    # 3. datasets & loaders
    tr_ds = SequenceDataset(tr_df, feature_cols, seq_len)
    val_ds = SequenceDataset(val_df, feature_cols, seq_len)
    test_ds = SequenceDataset(test_df, feature_cols, seq_len)

    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, drop_last=False)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)

    input_dim = len(feature_cols)
    if model_type == "lstm":
        model = LSTMPredictor(input_dim)
    elif model_type == "trans":
        model = TransformerPredictor(input_dim)
    else:
        raise ValueError("model_type must be 'lstm' or 'trans'")

    model, _ = train_model(model, tr_loader, val_loader, device=device)

    # 4.  inference on test & write back
    test_df = test_df.copy()
    test_df["pred"] = np.nan
    model.eval()
    with torch.no_grad():
        for X, _, idx in test_loader:
            X = X.to(device)
            pred = model(X).cpu().numpy().flatten()
            test_df.loc[idx.tolist(), "pred"] = pred

    mask = ~test_df["pred"].isna()
    test_mae = mean_absolute_error(test_df.loc[mask, "y_clip"], test_df.loc[mask, "pred"])
    print("Test MAE:", test_mae)
    return model, test_df, test_mae


if __name__ == "__main__":
    # Example usage (expects you already prepared dataframes):
    model, test_df_out, mae = run_pipeline(

    train_df, test_df, feature_cols, model_type="trans", device="cpu"

)
    #pass

报错：---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[120], line 326
    321     return model, test_df, test_mae
    324 if __name__ == "__main__":
    325     # Example usage (expects you already prepared dataframes):
--> 326     model, test_df_out, mae = run_pipeline(
    327 
    328     train_df, test_df, feature_cols, model_type="trans", device="cpu"
    329 
    330 )
    331     #pass

Cell In[120], line 316, in run_pipeline(train_df, test_df, feature_cols, seq_len, batch_size, model_type, device)
    314         X = X.to(device)
    315         pred = model(X).cpu().numpy().flatten()
--> 316         test_df.loc[idx.tolist(), "pred"] = pred
    318 mask = ~test_df["pred"].isna()
    319 test_mae = mean_absolute_error(test_df.loc[mask, "y_clip"], test_df.loc[mask, "pred"])

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\indexing.py:818, in _LocationIndexer.__setitem__(self, key, value)
    815 self._has_valid_setitem_indexer(key)
    817 iloc = self if self.name == "iloc" else self.obj.iloc
--> 818 iloc._setitem_with_indexer(indexer, value, self.name)

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\indexing.py:1795, in _iLocIndexer._setitem_with_indexer(self, indexer, value, name)
   1792 # align and set the values
   1793 if take_split_path:
   1794     # We have to operate column-wise
-> 1795     self._setitem_with_indexer_split_path(indexer, value, name)
   1796 else:
   1797     self._setitem_single_block(indexer, value, name)

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\indexing.py:1838, in _iLocIndexer._setitem_with_indexer_split_path(self, indexer, value, name)
   1834     self._setitem_with_indexer_2d_value(indexer, value)
   1836 elif len(ilocs) == 1 and lplane_indexer == len(value) and not is_scalar(pi):
   1837     # We are setting multiple rows in a single column.
-> 1838     self._setitem_single_column(ilocs[0], value, pi)
   1840 elif len(ilocs) == 1 and 0 != lplane_indexer != len(value):
   1841     # We are trying to set N values into M entries of a single
   1842     #  column, which is invalid for N != M
   1843     # Exclude zero-len for e.g. boolean masking that is all-false
   1845     if len(value) == 1 and not is_integer(info_axis):
   1846         # This is a case like df.iloc[:3, [1]] = [0]
   1847         #  where we treat as df.iloc[:3, 1] = 0

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\indexing.py:1992, in _iLocIndexer._setitem_single_column(self, loc, value, plane_indexer)
   1988         value = value[pi]
   1989 else:
   1990     # set value into the column (first attempting to operate inplace, then
   1991     #  falling back to casting if necessary)
-> 1992     self.obj._mgr.column_setitem(loc, plane_indexer, value)
   1993     self.obj._clear_item_cache()
   1994     return

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\internals\managers.py:1391, in BlockManager.column_setitem(self, loc, idx, value, inplace)
   1389     col_mgr.setitem_inplace(idx, value)
   1390 else:
-> 1391     new_mgr = col_mgr.setitem((idx,), value)
   1392     self.iset(loc, new_mgr._block.values, inplace=True)

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\internals\managers.py:393, in BaseBlockManager.setitem(self, indexer, value)
    388 if _using_copy_on_write() and not self._has_no_reference(0):
    389     # if being referenced -> perform Copy-on-Write and clear the reference
    390     # this method is only called if there is a single block -> hardcoded 0
    391     self = self.copy()
--> 393 return self.apply("setitem", indexer=indexer, value=value)

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\internals\managers.py:352, in BaseBlockManager.apply(self, f, align_keys, ignore_failures, **kwargs)
    350         applied = b.apply(f, **kwargs)
    351     else:
--> 352         applied = getattr(b, f)(**kwargs)
    353 except (TypeError, NotImplementedError):
    354     if not ignore_failures:

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\internals\blocks.py:973, in Block.setitem(self, indexer, value)
    971 value = extract_array(value, extract_numpy=True)
    972 try:
--> 973     casted = np_can_hold_element(values.dtype, value)
    974 except LossySetitemError:
    975     # current dtype cannot store value, coerce to common dtype
    976     nb = self.coerce_to_target_dtype(value)

File c:\Users\rxliu\.conda\envs\project1\Lib\site-packages\pandas\core\dtypes\cast.py:2027, in np_can_hold_element(dtype, element)
   2023     raise LossySetitemError
   2024 elif not isinstance(tipo, np.dtype):
   2025     # i.e. nullable IntegerDtype or FloatingDtype;
   2026     #  we can put this into an ndarray losslessly iff it has no NAs
-> 2027     if element._hasna:
   2028         raise LossySetitemError
   2029     return element

AttributeError: 'numpy.ndarray' object has no attribute '_hasna'
