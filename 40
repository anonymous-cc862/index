你的目标是：**保持原有数据输入输出格式和训练/测试划分方式不变，将 LightGBM 预测部分用 LSTM 和 Transformer 两种深度学习模型替换**，用于预测 `y_clip`（目标变量），并输出测试集 MAE。

下面提供**详细、对齐原有 LightGBM 逻辑**的 PyTorch LSTM 和 Transformer 端到端流程。

---

# 1. 数据处理（按 theme 构造滑动窗口）

与 LightGBM 不同，LSTM/Transformer 需要时间序列特征。你的每行数据包含 `theme`、`date`、特征列、目标列，**我们按 theme 分组滑窗生成序列**。

### 主要思路

* 每支 theme 的数据按时间排序。
* 构造长度为 `seq_len` 的滑动窗口，每个窗口对应未来一天的 `y_clip`。
* 输入 shape: `[batch, seq_len, num_features]`，目标 shape: `[batch]`。

---

## 2. 实现代码

假设你的 `feature_cols` 已经是数值型（如有类别需 one-hot/categorical embedding，可进一步扩展）。

### 2.1 数据预处理和 Dataset 构造

```python
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler

SEQ_LEN = 10  # 窗口长度，可调整

# 先 fit scaler 只用 train_df
scaler = StandardScaler()
train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
test_df[feature_cols]  = scaler.transform(test_df[feature_cols])

class SequenceDataset(Dataset):
    def __init__(self, df, feature_cols, seq_len=SEQ_LEN):
        self.sequences = []
        self.targets = []
        for theme, group in df.groupby("theme"):
            group = group.sort_values("date")
            X = group[feature_cols].values
            y = group["y_clip"].values
            for i in range(len(group) - seq_len):
                self.sequences.append(X[i:i+seq_len])
                self.targets.append(y[i+seq_len])
    def __len__(self):
        return len(self.sequences)
    def __getitem__(self, idx):
        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)

train_dataset = SequenceDataset(train_df, feature_cols, seq_len=SEQ_LEN)
test_dataset  = SequenceDataset(test_df, feature_cols, seq_len=SEQ_LEN)
train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader   = DataLoader(test_dataset, batch_size=256, shuffle=False)
```

---

### 2.2 LSTM/Transformer模型定义

```python
import torch.nn as nn

class LSTMPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
    def forward(self, x):
        out, _ = self.lstm(x)
        pred = self.fc(out[:, -1, :])
        return pred.squeeze(-1)

class TransformerPredictor(nn.Module):
    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2, dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_model*4, dropout, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.fc = nn.Linear(d_model, 1)
    def forward(self, x):
        x = self.input_proj(x)
        out = self.transformer(x)
        pred = self.fc(out[:, -1, :])
        return pred.squeeze(-1)
```

---

### 2.3 训练与推理函数

```python
def train_model(model, train_loader, test_loader, num_epochs=20, lr=1e-3, device='cuda'):
    model = model.to(device)
    criterion = nn.L1Loss()  # MAE 对齐 LightGBM 的 l1
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    best_mae = float("inf")
    for epoch in range(num_epochs):
        model.train()
        for X, y in train_loader:
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            pred = model(X)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
        # 验证
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for X, y in test_loader:
                X, y = X.to(device), y.to(device)
                pred = model(X)
                y_true.append(y.cpu().numpy())
                y_pred.append(pred.cpu().numpy())
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        print(f"Epoch {epoch+1}, Test MAE: {mae:.6f}")
        if mae < best_mae:
            best_mae = mae
            best_w = model.state_dict()
    model.load_state_dict(best_w)
    return model
```

---

### 2.4 训练与评估

以 LSTM 为例：

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model = LSTMPredictor(input_dim=len(feature_cols))
model = train_model(model, train_loader, test_loader, num_epochs=20, lr=3e-4, device=device)

# 推理
model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for X, y in test_loader:
        X, y = X.to(device), y.to(device)
        pred = model(X)
        y_true.append(y.cpu().numpy())
        y_pred.append(pred.cpu().numpy())
y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)
print("Test MAE:", mean_absolute_error(y_true, y_pred))
```

Transformer只需把`model = LSTMPredictor(...)`换成`model = TransformerPredictor(input_dim=len(feature_cols))`即可。

---

## 总结要点

* **输入输出和 LightGBM 一致**，只不过深度学习模型是序列建模，特征为滑动窗口输入。
* **指标仍然是 MAE**，可以直接对比 LightGBM。
* **适配多 theme，支持窗口滑动生成序列**。

---

如果你有**单行/非序列场景**，只需把`seq_len=1`即可，代码自动退化为MLP/传统机器学习输入格式。

如有需要**完整工程版**（如带EarlyStopping/模型保存/支持多目标/类别特征embedding/与回测代码自动集成），可继续提需求！
