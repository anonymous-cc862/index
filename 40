当然可以！下面直接给出你**实际可运行**、对齐原表**每行都一一对应**的 LSTM/Transformer 训练与预测流程，包括**滑窗补齐、批量预测和对齐回填**，只需替换你的原 LightGBM 部分即可。

---

## 1. 数据滑窗 Dataset & 标准Scaler

```python
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

SEQ_LEN = 10  # 可调
BATCH_SIZE = 128

# 训练/测试集都要提前标准化（只用train fit scaler！）
scaler = StandardScaler()
train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
test_df[feature_cols]  = scaler.transform(test_df[feature_cols])

class SequenceDataset(Dataset):
    def __init__(self, df, feature_cols, seq_len=SEQ_LEN):
        self.seq_len = seq_len
        self.feature_cols = feature_cols
        self.data = []
        self.target = []
        self.group_idx = []  # 记录每个样本在原表的位置
        for theme, group in df.groupby("theme"):
            group = group.sort_values("date")
            X = group[feature_cols].values
            y = group["y_clip"].values
            idx = group.index.values
            for i in range(len(group) - seq_len):
                self.data.append(X[i:i+seq_len])
                self.target.append(y[i+seq_len])
                self.group_idx.append(idx[i+seq_len])
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return (torch.tensor(self.data[idx], dtype=torch.float32),
                torch.tensor(self.target[idx], dtype=torch.float32),
                self.group_idx[idx])  # 返回在原表中的行号
```

---

## 2. LSTM/Transformer模型定义

```python
import torch.nn as nn

class LSTMPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
    def forward(self, x):
        out, _ = self.lstm(x)
        pred = self.fc(out[:, -1, :])
        return pred.squeeze(-1)

class TransformerPredictor(nn.Module):
    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2, dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_model*4, dropout, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.fc = nn.Linear(d_model, 1)
    def forward(self, x):
        x = self.input_proj(x)
        out = self.transformer(x)
        pred = self.fc(out[:, -1, :])
        return pred.squeeze(-1)
```

---

## 3. 训练和推理函数（带行号映射）

```python
def train_model(model, train_loader, val_loader, num_epochs=20, lr=3e-4, device='cuda'):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.L1Loss()
    best_mae = float('inf')
    best_state = None
    for epoch in range(num_epochs):
        model.train()
        for X, y, _ in train_loader:
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            pred = model(X)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
        # Validation
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for X, y, _ in val_loader:
                X, y = X.to(device), y.to(device)
                pred = model(X)
                y_true.append(y.cpu().numpy())
                y_pred.append(pred.cpu().numpy())
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        print(f"Epoch {epoch+1}, Valid MAE: {mae:.6f}")
        if mae < best_mae:
            best_mae = mae
            best_state = model.state_dict()
    model.load_state_dict(best_state)
    return model

# 构建Dataset和Loader
train_dataset = SequenceDataset(train_df, feature_cols, seq_len=SEQ_LEN)
test_dataset  = SequenceDataset(test_df,  feature_cols, seq_len=SEQ_LEN)
train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader   = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
```

---

## 4. 预测并**按原表行号映射回填**

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
# model = LSTMPredictor(input_dim=len(feature_cols))    # 用LSTM
model = TransformerPredictor(input_dim=len(feature_cols))  # 用Transformer

model = train_model(model, train_loader, test_loader, num_epochs=20, lr=3e-4, device=device)

# 预测test集，按原index还原
test_df = test_df.copy()
test_df["pred"] = np.nan

model.eval()
with torch.no_grad():
    for X, _, idx in test_loader:
        X = X.to(device)
        pred = model(X).cpu().numpy()
        test_df.loc[idx.numpy(), "pred"] = pred  # 根据原index赋值

# 评估
mask = ~test_df["pred"].isna()
mae = mean_absolute_error(test_df.loc[mask, "y_clip"], test_df.loc[mask, "pred"])
print("Test MAE:", mae)
```

---

### ⚠️ 这样你的 `test_df["pred"]` 行数和 test\_df 完全对齐，不会报错，**每行都能找回预测值或 NaN（滑窗前SEQ\_LEN行）**。

---

## 总结

* **Dataset每条记录带原行号**，预测后直接回填到test\_df
* 避免长度不一致和报错，且可以直接与原表做下游回测
* 可选补充：`test_df["pred"].fillna(0)` 或其它方式对NaN补全

如需要**多 theme、多窗口、单点特征等其它自定义场景**，也都可用类似的对齐策略处理。
如果你需要更复杂的批量推理代码（支持更大的数据，自动对不同theme滑窗等），可以继续提问！

---

如需要**完整代码一键运行文件**，请告知！
