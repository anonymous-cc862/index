您好，这是一个非常重要且核心的问题，属于机器学习模型的可解释性（Interpretability）范畴。对于一个已经训练好的决策树，我们想知道某个样本的预测结果具体是由哪些特征以及如何决定的。

主要有三种方法，从最直接到最强大，我将为您一一介绍。

---

### 方法一：最直接的方式 —— 可视化决策路径

对于**单个决策树**（而非随机森林等集成模型），最直观的方法就是跟踪这个样本在树中走过的路径。路径上的每一个节点都是一个决策，共同导向了最终的预测结果。

#### 1. 工作原理
决策树的预测过程就是一系列的 “if-else” 判断。一个样本从根节点开始，根据其特征值满足哪个条件，被一步步地分配到左子树或右子树，直到抵达一个叶节点。这个叶节点的输出值就是该样本的最终预测值。

这条从根到叶的路径，就是对这个样本预测最精确的解释。

#### 2. 如何实现 (使用Scikit-learn)

假设您有训练好的模型`model`和一个样本`sample`（一个Pandas Series或NumPy array）。

```python
import pandas as pd
from sklearn.tree import DecisionTreeRegressor, plot_tree
import matplotlib.pyplot as plt

# 假设您已经有以下变量
# model: 训练好的 DecisionTreeRegressor 或 DecisionTreeClassifier
# X_train: 用于训练的特征DataFrame
# sample: 你想解释的那个样本 (必须是DataFrame格式，即使只有一行)

# --- 1. 可视化整个决策树 ---
plt.figure(figsize=(25, 15))
plot_tree(model, 
          feature_names=X_train.columns.tolist(), 
          filled=True, 
          rounded=True, 
          fontsize=10)
plt.title("Decision Tree Visualization")
plt.show()

# --- 2. 程序化地找出决策路径并解释 ---
feature = model.tree_.feature
threshold = model.tree_.threshold
leaf_values = model.tree_.value

# 获取样本通过的节点路径
node_indicator = model.decision_path(sample)
leaf_id = model.apply(sample)
node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]

print(f"样本的预测值为: {leaf_values[leaf_id[0]][0][0]:.4f}")
print("-" * 50)
print("决策路径如下:\n")

for node_id in node_index:
    # 如果不是叶节点
    if leaf_id[0] != node_id:
        feature_name = X_train.columns[feature[node_id]]
        threshold_value = threshold[node_id]
        sample_value = sample[feature_name].iloc[0]
        
        # 判断样本往左走还是往右走
        if sample_value <= threshold_value:
            inequality_sign = "<="
        else:
            inequality_sign = ">"
            
        print(f"节点 {node_id}: 如果 ( {feature_name} = {sample_value:.2f} ) "
              f"{inequality_sign} {threshold_value:.2f} -> 继续")
```

#### 3. 优缺点
* **优点**: 解释是100%精确和真实的，完全还原了模型的决策过程。
* **缺点**: 如果决策树非常深，路径会很长，人类难以完全理解。它只告诉你“走了哪条路”，但没有直观地量化每个决策对最终结果的“贡献度”。

---

### 方法二：黄金标准 —— 使用SHAP (SHapley Additive exPlanations)

SHAP 是目前业界公认的解释机器学习模型的最佳工具之一。它能告诉你**每个特征的每个取值，是如何将预测结果从“平均水平”推高或拉低的**。

#### 1. 工作原理（直观理解）
您可以把一次预测想象成一个合作游戏：
* **玩家**: 每个特征都是一个玩家。
* **游戏结果**: 最终的预测值。
* **平均奖金**: 所有样本的平均预测值（基准线）。
* **SHAP值**: 每个“玩家”（特征）对于“将游戏结果从平均奖金推到最终结果”这件事，做出了多大的贡献。

一个正的SHAP值意味着这个特征把预测值**推高**了，一个负的SHAP值意味着它把预测值**拉低**了。

#### 2. 如何实现

首先需要安装`shap`库: `pip install shap`

```python
import shap

# 假设您已经有以下变量
# model: 训练好的决策树模型
# X_train: 用于训练的特征DataFrame
# sample: 你想解释的那个样本 (DataFrame格式)

# 1. 创建一个针对树模型的SHAP解释器
explainer = shap.TreeExplainer(model)

# 2. 计算该样本的SHAP值
# 注意：shap_values的输出形状可能与模型有关，对于回归通常是一个数组
shap_values = explainer.shap_values(sample)

# 3. 可视化解释
# 这是最强大、最直观的部分！
print("SHAP 力图 (Force Plot):")
shap.initjs() # 初始化JS环境（在Jupyter Notebook中）

# explainer.expected_value 是所有训练样本预测的平均值（基准）
# shap_values[0] 是该样本各个特征的SHAP值
shap.force_plot(explainer.expected_value, 
                shap_values[0], 
                sample.iloc[0], 
                matplotlib=True) # 使用matplotlib=True可以在非notebook环境下显示
```
**力图 (Force Plot) 解读**:
* **base value**: 是所有样本的平均预测值。
* **红色特征**: 将预测值推高的特征，越长代表贡献越大。
* **蓝色特征**: 将预测值拉低的特征，越长代表贡献越大。
* **f(x)**: 是模型对这个样本的最终预测值。

这张图完美地回答了您的问题：“主要是哪几个feature推动了y的预测高低？”

#### 3. 优缺点
* **优点**: 精确量化每个特征的贡献度，可视化效果极佳，理论基础坚实，并且适用于几乎所有模型（不止是决策树）。
* **缺点**: 对于非树模型，计算可能较慢（但对决策树和LightGBM等有专门优化，速度很快）。

---

### 方法三：全局特征重要性（作为参考）

您也可以查看模型训练后得到的全局特征重要性，但这**不能直接解释单一样本**。

```python
# 获取全局特征重要性
importances = model.feature_importances_
feature_importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\n全局特征重要性:")
print(feature_importance_df.head())
```
**重要提醒**: 这个列表告诉你哪些特征在**整个数据集上平均来说**最重要，但对于你关心的**某一个特定样本**，一个全局重要的特征可能毫无影响，而一个全局不重要的特征可能起到了决定性作用。

### 总结与建议

| 方法 | 类型 | 优点 | 缺点 | 推荐场景 |
| :--- | :--- | :--- | :--- | :--- |
| **可视化决策路径** | 局部解释 | 100%精确，还原过程 | 树深时难以理解，无贡献度量化 | 理解简单、较浅的决策树 |
| **SHAP** | 局部解释 | **精确量化贡献度**，可视化极佳 | 概念稍抽象，需要安装库 | **强烈推荐！** 完美回答您的问题 |
| **全局特征重要性** | 全局解释 | 计算简单，快速概览 | **不能解释单一样本** | 了解模型在整体上依赖哪些特征 |

对于您“看主要是哪几个feature推动了y的预测高低”这个问题，**我最推荐使用方法二 (SHAP)**，因为它提供了最直观、量化的答案。方法一可以作为辅助，帮助您理解SHAP结果背后的具体决策规则。






import pandas as pd
import numpy as np
from itertools import product
from joblib import Parallel, delayed
from tqdm import tqdm
from scipy.stats import zscore
import os
import glob

# Assume EnhancedTimingStrategy class is defined here or imported
# from strategy_definition import EnhancedTimingStrategy, get_holding_days

# ────────────── HELPER: PERFORMANCE CALCULATION ──────────────

def calculate_performance_metrics(returns_series, periods_per_year=252):
    """
    Calculates key performance metrics for a given returns series.
    """
    if returns_series.empty or returns_series.sum() == 0:
        return 0, 0, 0, 0

    total_return = (1 + returns_series).prod()
    num_days = len(returns_series)
    annualized_return = total_return ** (periods_per_year / num_days) - 1

    annualized_vol = returns_series.std() * np.sqrt(periods_per_year)
    sharpe_ratio = annualized_return / annualized_vol if annualized_vol != 0 else 0

    # Calculate Max Drawdown
    cum_returns = (1 + returns_series).cumprod()
    peak = cum_returns.expanding(min_periods=1).max()
    drawdown = (cum_returns - peak) / peak
    max_drawdown = drawdown.min()

    # Calculate Rolling5_MaxDD (Average of 5 largest drawdowns)
    drawdown_periods = []
    is_in_drawdown = False
    for date, val in drawdown.items():
        if val < 0 and not is_in_drawdown:
            is_in_drawdown = True
            start_date = date
        elif val == 0 and is_in_drawdown:
            is_in_drawdown = False
            end_date = date
            min_drawdown_in_period = drawdown.loc[start_date:end_date].min()
            drawdown_periods.append(min_drawdown_in_period)
    
    if is_in_drawdown: # handle drawdown at the end of the series
        min_drawdown_in_period = drawdown.loc[start_date:].min()
        drawdown_periods.append(min_drawdown_in_period)

    largest_drawdowns = sorted(drawdown_periods)[:5] # Get the 5 smallest (most negative) values
    rolling5_maxdd = np.mean(largest_drawdowns) if largest_drawdowns else 0


    return annualized_return, sharpe_ratio, max_drawdown, rolling5_maxdd


# ────────────── MAIN PROCESSING FUNCTION ──────────────

def process_asset_file(file_path):
    """
    Main function to run the entire backtesting pipeline for a single asset file.
    """
    try:
        theme = os.path.basename(file_path).split('_')[0]
        print(f"\n{'='*20} Processing Theme: {theme} {'='*20}")

        # ────────────── 1. Data Preparation ──────────────
        df = pd.read_csv(file_path)
        df["date"] = pd.to_datetime(df["date"])
        df = df[~((df["date"].dt.weekday >= 5) & (df["weighted_return_2"] == 0))]
        daily_rets = df.set_index("date")["weighted_return_2"].sort_index()
        
        # Ensure there is data to process
        if daily_rets.empty:
            print(f"Skipping {theme}: No data after filtering.")
            return

        train_end = pd.Timestamp("2024-07-01")
        in_sample = daily_rets.loc[: train_end - pd.Timedelta(days=1)]
        out_sample = daily_rets.loc[train_end:]
        
        if in_sample.empty or out_sample.empty:
            print(f"Skipping {theme}: Not enough data for train/test split.")
            return

        # ────────────── 2. Grid Search Parameters ──────────────
        signal_grid = {
            "pct_window": [10, 30],
            "rsi_lookback": [5],
            "RSI_pct_low": [0.1, 0.2, 0.3],
            "RSI_pct_high": [0.9, 0.8, 0.7],
            "EMA100_slope_pct_high": [0.9, 0.8, 0.7],
            "EMA100_slope_pct_low": [0.3, 0.2, 0.1],
            "base_risk": [0.1, 0.2, 0.3],
        }
        stoploss_grid = [300, 400]
        takeprofit_grid = [300]
        sig_names = list(signal_grid.keys())
        sig_vals = list(signal_grid.values())
        signal_combos = list(product(*sig_vals))

        # ────────────── 3. First Round: Signal Parameter Search ──────────────
        print(f"Round 1 Parallel Search: {len(signal_combos)} combinations for {theme}")
        sig_res = Parallel(-1, backend="loky")(
            delayed(eval_signal)(c, in_sample, sig_names) for c in tqdm(signal_combos)
        )
        sig_clean = [r for r in sig_res if r]
        if not sig_clean:
            raise RuntimeError(f"Round 1 failed for {theme}, no results!")
        
        sig_df = pd.DataFrame(sig_clean)
        sig_df["utility"] = (0.9 * zscore(sig_df["sharpe"]) - 0.1 * zscore(sig_df["maxdd"].abs()))
        top12 = sig_df.nlargest(12, "utility").to_dict("records")
        
        # ────────────── 4. Second Round: Stop-Loss / Take-Profit Search ──────────────
        tasks = [(p, sl, tp) for p in top12 for sl in stoploss_grid for tp in takeprofit_grid]
        print(f"Round 2 Parallel Search: {len(tasks)} combinations for {theme}")
        stop_res = Parallel(-1, backend="loky")(
            delayed(eval_stoploss)(p, sl, tp, in_sample, sig_names) for p, sl, tp in tqdm(tasks)
        )
        stop_clean = [r for r in stop_res if r]
        if not stop_clean:
            raise RuntimeError(f"Round 2 failed for {theme}, no results!")

        stop_df = pd.DataFrame(stop_clean)
        stop_df["utility"] = (0.9 * zscore(stop_df["sharpe"]) - 0.1 * zscore(stop_df["maxdd"].abs()))
        best = stop_df.loc[stop_df["utility"].idxmax()]

        # ────────────── 5. Output Generation ──────────────
        # Create directories
        picture_folder = os.path.join("picture_signal1", theme)
        os.makedirs(picture_folder, exist_ok=True)
        os.makedirs("performance_signal1", exist_ok=True)

        results_list = []

        # --- Train Set Evaluation ---
        print(f"\n===== Evaluating best parameters on IN-SAMPLE for {theme} =====")
        strat_train = EnhancedTimingStrategy(in_sample)
        set_params(strat_train, best, fix_tp_sl=False)
        strat_train.calc_indicators()
        strat_train.generate_signals1()
        strat_train.backtest1()
        train_plot_path = os.path.join(picture_folder, f"{theme}_Train_Backtest.png")
        ann_train, shp_train, mdd_train, _, results_df_train = strat_train.performance(plot_trades=True, save_path=train_plot_path)
        
        # Get holding days stats from strategy results
        h_days_info = get_holding_days(results_df_train)
        
        # Calculate strategy metrics
        _, _, _, r5_mdd_train = calculate_performance_metrics(results_df_train["ret"])

        results_list.append({
            "theme": theme, "period": "Train", "type": "Strategy",
            "Annualized Return": ann_train, "Sharpe": shp_train, "MaxDD": mdd_train,
            "Rolling5_MaxDD": r5_mdd_train, "avg_holding_days": h_days_info['avg'],
            "min_holding_days": h_days_info['min'], "max_holding_days": h_days_info['max']
        })

        # Calculate Buy-and-Hold metrics for Train set
        ann_bh_train, shp_bh_train, mdd_bh_train, r5_mdd_bh_train = calculate_performance_metrics(in_sample)
        results_list.append({
            "theme": theme, "period": "Train", "type": "Buy-and-Hold",
            "Annualized Return": ann_bh_train, "Sharpe": shp_bh_train, "MaxDD": mdd_bh_train,
            "Rolling5_MaxDD": r5_mdd_bh_train, "avg_holding_days": len(in_sample),
            "min_holding_days": len(in_sample), "max_holding_days": len(in_sample)
        })

        # --- Test Set Evaluation ---
        print(f"\n===== Evaluating best parameters on OUT-OF-SAMPLE for {theme} =====")
        strat_test = EnhancedTimingStrategy(out_sample)
        set_params(strat_test, best, fix_tp_sl=False)
        strat_test.calc_indicators()
        strat_test.generate_signals1()
        strat_test.backtest1()
        test_plot_path = os.path.join(picture_folder, f"{theme}_Test_Backtest.png")
        ann_test, shp_test, mdd_test, _, results_df_test = strat_test.performance(plot_trades=True, save_path=test_plot_path)

        # Get holding days stats
        h_days_info_test = get_holding_days(results_df_test)
        
        # Calculate strategy metrics
        _, _, _, r5_mdd_test = calculate_performance_metrics(results_df_test["ret"])

        results_list.append({
            "theme": theme, "period": "Test", "type": "Strategy",
            "Annualized Return": ann_test, "Sharpe": shp_test, "MaxDD": mdd_test,
            "Rolling5_MaxDD": r5_mdd_test, "avg_holding_days": h_days_info_test['avg'],
            "min_holding_days": h_days_info_test['min'], "max_holding_days": h_days_info_test['max']
        })

        # Calculate Buy-and-Hold metrics for Test set
        ann_bh_test, shp_bh_test, mdd_bh_test, r5_mdd_bh_test = calculate_performance_metrics(out_sample)
        results_list.append({
            "theme": theme, "period": "Test", "type": "Buy-and-Hold",
            "Annualized Return": ann_bh_test, "Sharpe": shp_bh_test, "MaxDD": mdd_bh_test,
            "Rolling5_MaxDD": r5_mdd_bh_test, "avg_holding_days": len(out_sample),
            "min_holding_days": len(out_sample), "max_holding_days": len(out_sample)
        })

        # --- Save performance data to CSV ---
        performance_df = pd.DataFrame(results_list)
        performance_csv_path = os.path.join("performance_signal1", f"{theme}.csv")
        performance_df.to_csv(performance_csv_path, index=False)
        print(f"\nPerformance data for {theme} saved to {performance_csv_path}")

    except Exception as e:
        print(f"An error occurred while processing {file_path}: {e}")

# ────────────── UTILITY FUNCTIONS (Modified for pipeline) ──────────────

def set_params(strat, p, fix_tp_sl=True):
    for k, v in p.items():
        if k in {"pct_window", "rsi_lookback"}:
            v = int(v) if v is not None else 1
            v = max(v, 1)
        setattr(strat, k, v)
    if fix_tp_sl:
        strat.stoploss_atr = 100
        strat.takeprofit_atr = 100

def get_holding_days(results):
    if "pos" not in results.columns:
        return {'avg': 0, 'min': 0, 'max': 0}
    trades, pos, entry_date = [], 0, None
    for dt, row in results.iterrows():
        if pos == 0 and row["pos"] != 0:
            pos, entry_date = row["pos"], dt
        elif pos != 0 and row["pos"] == 0:
            trades.append((dt - entry_date).days)
            pos = 0
    if trades:
        return {'avg': np.mean(trades), 'min': np.min(trades), 'max': np.max(trades)}
    return {'avg': 0, 'min': 0, 'max': 0}


def eval_signal(combo, in_sample_data, sig_names):
    p = dict(zip(sig_names, combo))
    strat = EnhancedTimingStrategy(in_sample_data)
    set_params(strat, p, fix_tp_sl=True)
    try:
        strat.calc_indicators()
        strat.generate_signals1()
        strat.backtest1()
        ann, shp, mdd, _, _ = strat.performance(plot_trades=False)
        return {**p, "sharpe": shp, "maxdd": mdd}
    except Exception:
        return None

def eval_stoploss(base_p, sl, tp, in_sample_data, sig_names):
    p = {**base_p, "stoploss_atr": sl, "takeprofit_atr": tp}
    strat = EnhancedTimingStrategy(in_sample_data)
    set_params(strat, p, fix_tp_sl=False)
    try:
        strat.calc_indicators()
        strat.generate_signals1()
        strat.backtest1()
        ann, shp, mdd, _, _ = strat.performance(plot_trades=False)
        return {**p, "sharpe": shp, "maxdd": mdd}
    except Exception:
        return None

# ────────────── SCRIPT ENTRY POINT ──────────────

if __name__ == "__main__":
    # Define the directory containing the asset files
    input_directory = "weighted_return"
    
    # Find all CSV files in the directory
    all_files = glob.glob(os.path.join(input_directory, "*.csv"))
    
    if not all_files:
        print(f"No CSV files found in directory: {input_directory}")
    else:
        print(f"Found {len(all_files)} files to process.")
        for file_path in all_files:
            process_asset_file(file_path)
    
    print("\nAll files processed.")
