import pandas as pd

import numpy as np

from scipy.stats import zscore

import ta            # pip install ta  (比 talib 更容易装；rsi/gvol 等函数名类似)
# ---------- 1. 
#数据准备 ----------

df = result_df1.copy()#df_clipped.copy()
df["date"] = pd.to_datetime(df["date"])
df = df.sort_values(["theme", "date"])
price_source = "ret"             # ① 只有收益率就先用它累积近似价格

# 用收益率重建一个"伪价格"索引，初始价取 1

df["px"] = df.groupby("theme")[price_source].transform(
    lambda r: (1 + r).cumprod()
)
# ---------- 2. 
#现有特征 ------------
grp = df.groupby("theme", group_keys=False)

# 2-1 3 日累计收益 (rolling 累积)
df["ret_3d_m"] = grp["px"].apply(lambda s: s.rolling(3).mean())
df["px_3d_c"] = grp["px"].transform(lambda s: s / s.shift(3) - 1)#
df["ret_3d_std"] = grp["ret"].apply(lambda s: s.rolling(3).std())

# 2-3 RSI_14
df["rsi_14"] = grp["px"].apply(lambda s: ta.momentum.rsi(s, window=14))

# 2-4 成交量因子（volume / 20d均值）
df["vol_rel_20d"] = (
    df.groupby("theme")["volume"]
      .transform(lambda s: s / s.rolling(window=20, min_periods=10).mean())
)
df["vol_20d_mean"] = (
    df.groupby("theme")["volume"]
      .transform(lambda s: s.rolling(window=20, min_periods=10).mean())
)
#衍生特征 ----------
# 3-1 动量：5/10/20 d 累积收益
for k in (5, 10, 20):
    df[f"mom_{k}d"] = grp["px"].apply(lambda s: s.pct_change(k))

# 3-3 成交额 rolling-zscore
def rolling_z(series, w=20, min_p=10):
    ma = series.rolling(w, min_periods=min_p).mean()
    sd = series.rolling(w, min_periods=min_p).std()
    return (series - ma) / sd
df["turnover_z"] = df.groupby("theme")["turnover"].transform(rolling_z)

w_fast, w_slow = 5, 20
df['wma_fast'] = grp['px'].apply(lambda s: s.ewm(span=w_fast, min_periods=3, adjust=False).mean())
df['wma_slow'] = grp['px'].apply(lambda s: s.ewm(span=w_slow, min_periods=10, adjust=False).mean())
df['ma_spread'] = df['wma_fast'] / df['wma_slow'] - 1            # 均线差

# ③ 逐 theme 做 20 日滚动皮尔逊相关
df["ret"] = df.groupby("theme")["px"].pct_change()

theme_idx_ret = (
    df.groupby(["theme", "date"])["ret"]
      .mean()
      .rename("ret_idx")               # 每天每个 theme 的指数收益
      .reset_index()
)

# ② 合回原表，得到每行两列：个股 ret、指数 ret_idx
df = df.merge(theme_idx_ret, on=["theme", "date"], how="left")

def rolling_corr(g, window=20, min_p=10):
    return (
        g[["ret", "ret_idx"]]
        .rolling(window, min_periods=min_p)
        .corr()
        .unstack()               # ↘ 把 (ret, ret_idx) 两行转列
        .iloc[:, 1]              # 取 ret 与 ret_idx 的相关
    )

df["corr_20d"] = (
    df.sort_values("date")       # rolling 需要先按日期排序
      .groupby("theme", group_keys=False)
      .apply(rolling_corr)
)


ret_mkt = (           # 每天跨 theme 等权平均
    df.groupby("date")["ret"]
      .mean()
      .rename("ret_mkt")          # 当天市场指数收益
      .reset_index()
)

# 合回原表

df = df.merge(ret_mkt, on="date", how="left")
grp = df.groupby("theme", group_keys=False)   # ← 重新建！
# -------------------------------------------------------------
# ⑤ 相对收益：theme 指数 vs. 全市场
#     形式1：差值      -> ret_idx - ret_mkt
#     形式2：超额比率  -> (1+ret_idx)/(1+ret_mkt) - 1
# -------------------------------------------------------------
#f["rel_ret_idx"]  = df["ret_idx"] - df["ret_mkt"]               # 常用
df["rel_ret_ratio"] = (1 + df["ret_idx"]) / (1 + df["ret_mkt"]) - 1  # 若想要比率

# 布林带宽： (上轨-下轨)/中轨
def bollinger_bandwidth(s, n=20,min_p=10, k=2):
    mid = s.rolling(n, min_periods=min_p).mean()
    std = s.rolling(n, min_periods=min_p).std()
    upper, lower = mid + k*std, mid - k*std
    return (upper - lower) / mid
df['bb_width_20d'] = grp['px'].apply(bollinger_bandwidth)

df['ret_skew_20d'] = grp['ret'].apply(lambda s: s.rolling(20, min_periods=10).skew())

df['ret_kurt_20d'] = grp['ret'].apply(lambda s: s.rolling(20, min_periods=10).kurt())

def rolling_beta_alpha(sub, w=30, min_p=10):

    x = sub['ret_mkt']

    y = sub['ret']

    beta  = y.rolling(w, min_periods=min_p).cov(x) / x.rolling(w, min_periods=min_p).var()

    alpha = y.rolling(w, min_periods=min_p).mean() - beta * x.rolling(w, min_periods=min_p).mean()

    return pd.DataFrame({'beta_30d': beta, 'alpha_30d': alpha})

df[['beta_30d', 'alpha_30d']] = grp.apply(rolling_beta_alpha)   # 不再 droplevel


hi52 = grp['px'].apply(lambda s: s.rolling(60, min_periods=10).max())
lo52 = grp['px'].apply(lambda s: s.rolling(60, min_periods=10).min())
df['pct_to_hi52'] = (df['px'] - hi52) / hi52
df['pct_to_lo52'] = (df['px'] - lo52) / lo52

df['px_chg1d']  = grp['px'].pct_change()
df['vol_chg1d'] = grp['volume'].pct_change()
df['up_px_up_vol'] = ((df['px_chg1d'] > 0) & (df['vol_chg1d'] > 0)).astype(int)
# OBV（On-Balance Volume）
df['obv'] = grp.apply(lambda g: (np.sign(g['px_chg1d']) * g['volume']).cumsum()).values
df['obv_z20'] = grp['obv'].apply(lambda s: (s - s.rolling(20,min_periods=10).mean()) / s.rolling(20,min_periods=10).std())

# # ---------- 8.时间/日历效应 ------------------------------

df['dow'] = df['date'].dt.dayofweek          # 周几 [0,4]
for d in range(5):
    df[f'dow_{d}'] = (df['dow'] == d).astype(int)
df['eom'] = (df['date'] + pd.tseries.offsets.BMonthEnd(0) ==
             df['date']).astype(int)         # 月末 dummy
# ---------- 4. 
#收尾 ----------
df["y_5d"] = (
    df.groupby("theme")["px"]
      .apply(lambda s: s.shift(-5) / s - 1)    # shift(-5) 表示向前看 5 天
)

df['date']=pd.to_datetime(df['date'])

df_clean=df[['date','theme', 'ret', 'volume', 'turnover', 'crowd', 'crowd_c', 'mom_1m', 'mean_corr_5d', "corr_20d","px_3d_c",
        'ret_3d_m', 'ret_3d_std', 'rsi_14', 'vol_rel_20d',
       'mom_5d', 'mom_10d', 'mom_20d', 'turnover_z', 'wma_fast','wma_slow', 'ma_spread',
       'bb_width_20d','ret_skew_20d','ret_kurt_20d','beta_30d', 'alpha_30d',
       'pct_to_hi52', 'pct_to_lo52','px_chg1d', 'vol_chg1d', 'up_px_up_vol', 'obv', 'obv_z20', 'dow','dow_0', 'dow_1', 'dow_2', 'dow_3', 'dow_4', 'eom',
       'y_5d']]

df_clean=df_clean.dropna()

df_clean

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

df_clean = df_clean.copy()                       # 你的清洗结果
df_clean["date"] = pd.to_datetime(df_clean["date"])    # 确保日期是 datetime64

# ---------- 1. 划分 train / test ----------
cutoff = pd.Timestamp("2024-08-15")
train_df = df_clean[df_clean["date"] <= cutoff].copy()
test_df  = df_clean[df_clean["date"] >  cutoff].copy()

# ---------- 2. 计算 per-theme 分位 & clip ----------

# a) 计算分位（只看 train）
q = (train_df.groupby("theme")["y_5d"]
                .quantile([0.001, 0.999])
                .unstack(level=1)
                .rename(columns={0.001: "low", 0.999: "high"}))

# b) 把分位 merge 回两份数据
train_df = train_df.merge(q, on="theme", how="left")
test_df  = test_df.merge(q, on="theme", how="left")

# c) clip
train_df["y_clip"] = train_df["y_5d"].clip(train_df["low"], train_df["high"])
test_df["y_clip"]  = test_df["y_5d"].clip(test_df["low"], test_df["high"])


fix_cols = ["theme", "crowd_c"]          # 目前报错的两列

for col in fix_cols:
    if col == "theme":
        # 1) 作为类别特征，直接转 category
        train_df[col] = train_df[col].astype("category")
        test_df[col]  = test_df[col].astype("category")
    else:
        # 2) 其余 object 字符串列，尝试转成 float
        train_df[col] = pd.to_numeric(train_df[col], errors="coerce")
        test_df[col]  = pd.to_numeric(test_df[col], errors="coerce")

# （如果某列转成数值后产生 NaN，可再用 fillna 或 dropna 处理）

# train_df[col].fillna(0, inplace=True)

# ---------- 重新构造数据集 ----------

feature_cols = [
    c for c in train_df.columns
    if c not in ["date", "y_5d", "y_clip", "low", "high",'theme']
]

#cat_cols = ["theme"]                     # 只剩真正的类别列

train_set = lgb.Dataset(
    train_df[feature_cols], label=train_df["y_clip"]#,
   # categorical_feature=cat_cols
)

valid_set = lgb.Dataset(
    test_df[feature_cols],  label=test_df["y_clip"]#,
   # categorical_feature=cat_cols
)

# ---------- 训练 ----------
# # huber
params = {
    "objective": "huber",
    "alpha": 0.9,
    #"huber_delta": 0.00001,
    "learning_rate": 0.03,
    "num_leaves": 64,
    "feature_fraction": 0.7,
    "bagging_fraction": 0.8,
    "bagging_freq": 10,
    "lambda_l2": 2,
    "metric": "l1",
    "verbose": -1,
    "seed": 42
}

model = lgb.train(
    params,
    train_set,
    num_boost_round=2000,
    valid_sets=[valid_set],
    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]
)

# ---------- 推理 & 评估 ----------

y_pred = model.predict(test_df[feature_cols], num_iteration=model.best_iteration)
我现在的信号根据这个模型的预测结果产生，现在有result_df1的所有train和test data，如何构建一个回测框架?如何设定信号到什么时候买入什么时候卖出？买入什么仓位卖出什么仓位？回测的train是在result_df1训练预测模型的train上，test是在预测模型的test上吗？
