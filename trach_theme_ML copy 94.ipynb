{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b6718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests  \n",
    "import json\n",
    "import glob\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652c266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55473acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, os\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------- 可选：把你的函数做成一个独立模块 -----------------\n",
    "def process_one_file(csv_path: Path, out_dir: Path):\n",
    "    \"\"\"\n",
    "    读取 benchmark_weights/{theme}.csv\n",
    "    运行整段计算\n",
    "    保存结果到 weighted_return/{theme}_benchmark_weights.csv\n",
    "    \"\"\"\n",
    "    theme = csv_path.stem        # 例如 \"Gold\"\n",
    "    print(f\"=== Deal with {theme} ===\")\n",
    "\n",
    "    # ---------- 1) 读入权重 ----------\n",
    "    gold = pd.read_csv(csv_path)\n",
    "    #gold=pd.read_csv('benchmark_weights\\Gold_benchmark_weights.csv')\n",
    "    weight_data=gold.copy()#pd.read_csv(f\"benchmark_weights\\{theme}_benchmark_weights.csv\")\n",
    "\n",
    "    r = requests.post('http://flowrider.prod.schonfeld.com:8000/api/data/mkt-data-history', json={\n",
    "            'bbg_list': gold['bbg'].drop_duplicates().tolist(),\n",
    "            'start_date': '2023-05-15',\n",
    "            'end_date': '2025-05-15'})\n",
    "    rt_1 = pd.DataFrame(r.json())\n",
    "\n",
    "    #missing stocks\n",
    "    gold_set = set(gold['bbg'])\n",
    "    rt_1_set = set(rt_1['bbg'])\n",
    "    #在gold中但不在price_1_set中\n",
    "    missing_stocks =  gold_set - rt_1_set \n",
    "    if not missing_stocks:                        # empty set / list / Series\n",
    "        rt_all_d = rt_1.copy()                    # nothing to fetch, keep original\n",
    "        #find missing stock from another API\n",
    "    else:\n",
    "        miss_stock_list = list(missing_stocks)#.tolist()\n",
    "\n",
    "        # Add \" Equity\" to each item\n",
    "        miss_stock_list_equity = [item + \" Equity\" for item in miss_stock_list]\n",
    "\n",
    "        # Mandatory fields\n",
    "        securities = miss_stock_list_equity #['000046 CH Equity']\n",
    "        fields = ['CHG_PCT_1D']\n",
    "        start_date = 20230515\n",
    "        end_date = 20250515\n",
    "\n",
    "        # Optional fields\n",
    "        # overrides = {}\n",
    "        overrides = {\n",
    "            'EQY_FUND_CRNCY': 'USD',\n",
    "\n",
    "        }\n",
    "        # options = {}\n",
    "        options = {\n",
    "            'nonTradingDayFillOption': 'ALL_CALENDAR_DAYS',\n",
    "            'nonTradingDayFillMethod': 'PREVIOUS_VALUE'\n",
    "        }\n",
    "\n",
    "        # Make the API call using Python's requests package\n",
    "        url = 'http://flowrider.prod.schonfeld.com:8005/bdh'\n",
    "        payload = {\n",
    "            'securities': securities,\n",
    "            'fields': fields,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'overrides': overrides,\n",
    "            'options': options\n",
    "        }\n",
    "        response = requests.post(url, json=payload)\n",
    "        rt_miss_d=pd.DataFrame(response.json()['content'])\n",
    "\n",
    "        # Make a copy of rt_miss to avoid modifying the original\n",
    "        rt_miss_transformed = rt_miss_d.copy()\n",
    "        rt_miss_transformed['date'] = pd.to_datetime(rt_miss_transformed['date']).dt.strftime('%Y-%m-%d')\n",
    "        rt_miss_transformed['security'] = rt_miss_transformed['security'].str.replace(' Equity', '')\n",
    "        rt_miss_transformed = rt_miss_transformed.rename(columns={'security': 'bbg'})\n",
    "        rt_miss_transformed = rt_miss_transformed.rename(columns={'CHG_PCT_1D': 'px_chg_1d'})\n",
    "\n",
    "        # Divide the px_chg_1d values by 10\n",
    "        rt_miss_transformed['px_chg_1d'] = pd.to_numeric(rt_miss_transformed['px_chg_1d'], errors='coerce')\n",
    "        rt_miss_transformed['px_chg_1d'] = rt_miss_transformed['px_chg_1d'] / 100\n",
    "\n",
    "        #concat with rt_1\n",
    "        rt_all_d=pd.concat([rt_1, rt_miss_transformed], ignore_index=True)\n",
    "        rt_all_d=rt_all_d.sort_values(by=['date','bbg']).reset_index(drop=True)\n",
    "\n",
    "    df_d=rt_all_d\n",
    "    theme_data = df_d\n",
    "\n",
    "\n",
    "    # # 创建透视表，行为日期，列为股票代码，值为价格变化\n",
    "    pivot_data = theme_data.pivot_table(\n",
    "        index='date',\n",
    "        columns='bbg',\n",
    "        values='px_chg_1d'#'px_chg_1w'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Calculate the two types of weighted returns\n",
    "    #equal_weighted_returns = pivot_data.apply(calculate_equal_weighted_return, axis=1)\n",
    "    def calculate_freefloat_weighted_return(date_row):\n",
    "        total_weighted_return = 0\n",
    "        \n",
    "        for bbg in date_row.index:\n",
    "            if pd.notna(date_row[bbg]):  # Check if return is not NaN\n",
    "                if bbg in weight_data['bbg'].values:\n",
    "                    weight = weight_data.loc[weight_data['bbg'] == bbg, 'benchmark_2_weight'].values[0]\n",
    "                    total_weighted_return += date_row[bbg] * weight\n",
    "        \n",
    "        return total_weighted_return\n",
    "\n",
    "    free_weighted_returns = pivot_data.apply(calculate_freefloat_weighted_return, axis=1)\n",
    "\n",
    "    # Create a new DataFrame with both columns\n",
    "    combined_returns_d = pd.DataFrame({\n",
    "        #'weighted_return_1': equal_weighted_returns,\n",
    "        'weighted_return_2': free_weighted_returns\n",
    "    })\n",
    "    # ---------- 5) 保存 ----------\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    out_path = out_dir / f\"{theme}_return.csv\"\n",
    "    combined_returns_d.to_csv(out_path, index=True)\n",
    "    print(f\"已保存: {out_path}\")\n",
    "\n",
    "# ----------------- 主脚本 -----------------\n",
    "def main():\n",
    "    in_dir  = Path(\"benchmark_weights\")\n",
    "    out_dir = Path(\"weighted_return\")\n",
    "\n",
    "    for csv_file in in_dir.glob(\"*.csv\"):\n",
    "        process_one_file(csv_file, out_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df=pd.read_csv('market_data_history_202505201802.csv')\n",
    "volume_df= volume_df.sort_values([\"date\", \"bbg\"])\n",
    "volume_df = volume_df[\n",
    "    (volume_df[\"date\"] >= \"2023-05-15\") & \n",
    "    (volume_df[\"date\"] <= \"2025-05-15\")\n",
    "]\n",
    "volume_df[\"date\"] = pd.to_datetime(volume_df[\"date\"])\n",
    "volume_df = volume_df.sort_values([\"bbg\", \"date\"])\n",
    "\n",
    "volume_df[\"vol_chg_pct\"] = (\n",
    "    volume_df\n",
    "        .groupby(\"bbg\")[\"volume\"]\n",
    "        .pct_change()         # (vol_t / vol_{t-1} - 1)\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "DIR_W = Path(\"benchmark_weights\")          # 目录路径，可改绝对路径\n",
    "\n",
    "suffix  = \"_benchmark_weights.csv\"         # 需要剔除的后缀\n",
    "\n",
    "bench_list = [\n",
    "\n",
    "    f.stem.replace(suffix[:-4], \"\")        # stem 去掉\".csv\"，再删「_benchmark_weights」\n",
    "\n",
    "    for f in DIR_W.glob(\"*_benchmark_weights.csv\")\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ========== 可选：要循环的 Benchmark 名称 ==========\n",
    "benchmarks = bench_list             # 需要几个就写几个，例如 [\"Gold\", \"Silver\", \"Oil\"]\n",
    "# ========== 可选：存放文件夹 ==========\n",
    "DIR_W  = Path(\"benchmark_weights\")\n",
    "DIR_R  = Path(\"weighted_return\")\n",
    "# ----------------------------------------------------\n",
    "def weighted_row_sum(row_vals, row_wts):\n",
    "    \"\"\"对一行做加权平均；只对非 NaN 的列参与计算\"\"\"\n",
    "    mask = row_vals.notna()\n",
    "    if mask.sum() == 0:\n",
    "        return pd.NA\n",
    "    adj_w = row_wts[mask]\n",
    "    adj_w = adj_w / adj_w.sum()            # 权重归一\n",
    "    return (row_vals[mask] * adj_w).sum()\n",
    "def run_one_benchmark(bench_name: str) -> pd.DataFrame | None:\n",
    "    \"\"\"返回单个 benchmark 的日度特征表 final_df（列：date, ret, volume, turnover,\n",
    "       crowd, crowd_c, mom_1m, theme）；若缺文件则返回 None\"\"\"\n",
    "    w_path = DIR_W / f\"{bench_name}_benchmark_weights.csv\"\n",
    "    r_path = DIR_R / f\"{bench_name}_benchmark_weights_return.csv\"\n",
    "    if not (w_path.exists() and r_path.exists()):\n",
    "        print(f\":warning:  {bench_name}: 找不到权重或收益文件，跳过\")\n",
    "        return None\n",
    "    # ---------- 基础数据 ----------\n",
    "    weight_df   = pd.read_csv(w_path)               # bbg, benchmark_2_weight\n",
    "    gold_return = pd.read_csv(r_path)               # date, bbg, weighted_return_2\n",
    "    weight_df[\"bbg\"]      = weight_df[\"bbg\"].astype(str)\n",
    "    gold_return[\"date\"]   = pd.to_datetime(gold_return[\"date\"])\n",
    "    idx_base = gold_return[\"date\"].sort_values().unique()  # 统一基准索引\n",
    "\n",
    "    # ========== ①  volume / turnover ==========\n",
    "    sub = (weight_df[[\"bbg\"]]\n",
    "           .merge(volume_df, on=\"bbg\", how=\"left\")\n",
    "           .sort_values([\"date\", \"bbg\"]))\n",
    "    # 行有效性判断：date 有值且 volume/turnover 至少一个非空\n",
    "    valid = sub[\"date\"].notna() & sub[[\"vol_chg_pct\"]].notna().any(axis=1)\n",
    "    sub = sub.loc[valid]\n",
    "    if sub.empty:\n",
    "        daily_feat_vol = pd.DataFrame(np.nan,\n",
    "                                      index=idx_base,\n",
    "                                      columns=[\"vol_chg_pct\"])\n",
    "    else:\n",
    "        vol_chg_pct_pivot = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "                                    values=\"vol_chg_pct\",   dropna=False)\n",
    "        # tur_pivot = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "        #                             values=\"turnover\", dropna=False)\n",
    "        # 统一重建权重矩阵\n",
    "        w_vec = (weight_df.set_index(\"bbg\")[\"benchmark_2_weight\"]\n",
    "                 .reindex(vol_chg_pct_pivot.columns))\n",
    "        weight_mat = pd.DataFrame(np.tile(w_vec.values, (len(vol_chg_pct_pivot), 1)),\n",
    "                                  index=vol_chg_pct_pivot.index,\n",
    "                                  columns=vol_chg_pct_pivot.columns)\n",
    "        idx_volume = vol_chg_pct_pivot.apply(\n",
    "            lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        # idx_turnov = tur_pivot.apply(\n",
    "        #     lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        daily_feat_vol = pd.DataFrame(\n",
    "            {\"vol_chg_pct\": idx_volume},\n",
    "            index=vol_chg_pct_pivot.index\n",
    "        ).reindex(idx_base)              # 填补缺失日期\n",
    "\n",
    "    # ========== ③  收益表 ==========\n",
    "    gold_daily_ret = (gold_return\n",
    "                    .pivot_table(index=\"date\",\n",
    "                                values=\"weighted_return_2\",\n",
    "                                aggfunc=\"first\")\n",
    "                    .reindex(idx_base))\n",
    "    # ========== ④  拼 final_df ==========\n",
    "    final_df = (gold_daily_ret\n",
    "                .join(daily_feat_vol, how=\"left\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"date\",\n",
    "                                \"weighted_return_2\": \"ret\"}))\n",
    "    final_df[\"theme\"] = bench_name\n",
    "    return final_df\n",
    "\n",
    "# ----------------------------------------------------\n",
    "parts = []\n",
    "for bench in bench_list:#[:3]:\n",
    "    print(f\"{bench} start\")\n",
    "    df = run_one_benchmark(bench)\n",
    "    if df is not None:\n",
    "        parts.append(df)\n",
    "    #print(f\"{bench} finish\")\n",
    "if not parts:\n",
    "    raise RuntimeError(\"没有拼接出任何结果！\")\n",
    "result_df = pd.concat(parts, ignore_index=True)\n",
    "print(f\":heavy_check_mark: 全部完成，result_df 行数: {len(result_df):,}\")\n",
    "print(result_df.head())\n",
    "result_df.to_csv('return_vol_chg_pct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95516212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27883b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DIR_W = Path(\"benchmark_weights\")          # 目录路径，可改绝对路径\n",
    "\n",
    "suffix  = \"_benchmark_weights.csv\"         # 需要剔除的后缀\n",
    "\n",
    "bench_list = [\n",
    "\n",
    "    f.stem.replace(suffix[:-4], \"\")        # stem 去掉\".csv\"，再删「_benchmark_weights」\n",
    "\n",
    "    for f in DIR_W.glob(\"*_benchmark_weights.csv\")\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------ 用户可改的参数 ------------\n",
    "\n",
    "ROOT = Path(r\"X:\\TeamRV-DON-US-RISK\\Intern\\Ruoxue\\data\\UBS\\CROWDING_SCORES\")\n",
    "\n",
    "START = datetime(2023, 5, 15)\n",
    "\n",
    "END   = datetime(2025, 5, 15)\n",
    "\n",
    "KEY   = \"ubs_crowding_scores_bloomberg_ticker\"\n",
    "\n",
    "USE_COLS = ['bloomberg_ticker', 'dt',\n",
    "\n",
    "            'crowding_factor', 'crowding_factor_change', 'mom_1m']\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "def file_date(path: Path) -> datetime | None:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    从文件名里提取形如 *_YYYYMMDD* 或 *_YYYYMM* 的日期，\n",
    "\n",
    "    返回 datetime；提取失败则返回 None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    m = re.search(r'_(\\d{6,8})$', path.stem)\n",
    "\n",
    "    if not m:\n",
    "\n",
    "        return None\n",
    "\n",
    "    ds = m.group(1)\n",
    "\n",
    "    if len(ds) == 8:\n",
    "\n",
    "        return datetime.strptime(ds, \"%Y%m%d\")\n",
    "\n",
    "    if len(ds) == 6:\n",
    "\n",
    "        return datetime.strptime(ds, \"%Y%m\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def read_one(path: Path) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    读取单文件，只保留 USE_COLS，自动识别 csv / excel，\n",
    "\n",
    "    并把列统一为小写\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if path.suffix.lower() in (\".xlsx\", \".xlsm\", \".xls\", \".xlsb\"):\n",
    "\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "    elif path.suffix.lower() == \".csv\":\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(f\"不支持的文件类型: {path}\")\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    missing = [c for c in USE_COLS if c not in df.columns]\n",
    "\n",
    "    if missing:\n",
    "\n",
    "        raise KeyError(f\"{path.name} 缺少列: {missing}\")\n",
    "\n",
    "    out = df[USE_COLS].copy()\n",
    "\n",
    "    # 若 dt 不是日期型，尝试解析\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(out['dt']):\n",
    "\n",
    "        out['dt'] = pd.to_datetime(out['dt'], errors='coerce')\n",
    "\n",
    "    out['src_file'] = path.name           # 方便溯源\n",
    "\n",
    "    return out\n",
    "\n",
    "# ------------ 主流程 ------------\n",
    "\n",
    "all_parts = []\n",
    "\n",
    "for p in ROOT.rglob(\"*\"):\n",
    "\n",
    "    if not p.is_file():\n",
    "\n",
    "        continue\n",
    "\n",
    "    name = p.name.lower()\n",
    "\n",
    "    if KEY not in name:\n",
    "\n",
    "        continue\n",
    "\n",
    "    d = file_date(p)\n",
    "\n",
    "    if d is None or not (START <= d <= END):\n",
    "\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "\n",
    "        part = read_one(p)\n",
    "\n",
    "        all_parts.append(part)\n",
    "\n",
    "        print(f\"✔ 读取 {p.name}   行数={len(part)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"⚠️  跳过 {p.name}  -- {e}\")\n",
    "\n",
    "if not all_parts:\n",
    "\n",
    "    raise RuntimeError(\"未找到满足条件的文件！\")\n",
    "\n",
    "crowd_concat = pd.concat(all_parts, ignore_index=True)\n",
    "\n",
    "print(\"\\n=== 合并完成 ===\")\n",
    "\n",
    "print(f\"总行数: {len(crowd_concat):,}\")\n",
    "\n",
    "print(crowd_concat.head())\n",
    "\n",
    "crowd_concat['bloomberg_ticker']=crowd_concat['bloomberg_ticker'].str.replace(' Equity','')\n",
    "crowd_concat=crowd_concat.drop(columns='src_file')\n",
    "crowd_concat['date'] = pd.to_datetime(crowd_concat['dt'])\n",
    "crowd_concat.sort_values('date', inplace=True)\n",
    "crowd_concat=crowd_concat[['date','bloomberg_ticker',  'crowding_factor', 'crowding_factor_change', 'mom_1m']]\n",
    "crowd_concat=crowd_concat.rename(columns={'bloomberg_ticker':'bbg'})\n",
    "crowd_concat_df=crowd_concat.sort_values([\"date\", \"bbg\"])\n",
    "crowd_concat_df = crowd_concat_df[\n",
    "    (crowd_concat_df[\"date\"] >= \"2023-05-15\") & \n",
    "    (crowd_concat_df[\"date\"] <= \"2025-05-15\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ========== 可选：要循环的 Benchmark 名称 ==========\n",
    "benchmarks = bench_list             # 需要几个就写几个，例如 [\"Gold\", \"Silver\", \"Oil\"]\n",
    "# ========== 可选：存放文件夹 ==========\n",
    "DIR_W  = Path(\"benchmark_weights\")\n",
    "DIR_R  = Path(\"weighted_return\")\n",
    "# ----------------------------------------------------\n",
    "def weighted_row_sum(row_vals, row_wts):\n",
    "    \"\"\"对一行做加权平均；只对非 NaN 的列参与计算\"\"\"\n",
    "    mask = row_vals.notna()\n",
    "    if mask.sum() == 0:\n",
    "        return pd.NA\n",
    "    adj_w = row_wts[mask]\n",
    "    adj_w = adj_w / adj_w.sum()            # 权重归一\n",
    "    return (row_vals[mask] * adj_w).sum()\n",
    "def run_one_benchmark(bench_name: str) -> pd.DataFrame | None:\n",
    "    \"\"\"返回单个 benchmark 的日度特征表 final_df（列：date, ret, volume, turnover,\n",
    "       crowd, crowd_c, mom_1m, theme）；若缺文件则返回 None\"\"\"\n",
    "    w_path = DIR_W / f\"{bench_name}_benchmark_weights.csv\"\n",
    "    r_path = DIR_R / f\"{bench_name}_benchmark_weights_return.csv\"\n",
    "    if not (w_path.exists() and r_path.exists()):\n",
    "        print(f\":warning:  {bench_name}: 找不到权重或收益文件，跳过\")\n",
    "        return None\n",
    "    # ---------- 基础数据 ----------\n",
    "    weight_df   = pd.read_csv(w_path)               # bbg, benchmark_2_weight\n",
    "    gold_return = pd.read_csv(r_path)               # date, bbg, weighted_return_2\n",
    "    weight_df[\"bbg\"]      = weight_df[\"bbg\"].astype(str)\n",
    "    gold_return[\"date\"]   = pd.to_datetime(gold_return[\"date\"])\n",
    "    idx_base = gold_return[\"date\"].sort_values().unique()  # 统一基准索引\n",
    "\n",
    "    # ========== ①  volume / turnover ==========\n",
    "    sub = (weight_df[[\"bbg\"]]\n",
    "           .merge(volume_df, on=\"bbg\", how=\"left\")\n",
    "           .sort_values([\"date\", \"bbg\"]))\n",
    "    # 行有效性判断：date 有值且 volume/turnover 至少一个非空\n",
    "    valid = sub[\"date\"].notna() & sub[[\"volume\", \"turnover\"]].notna().any(axis=1)\n",
    "    sub = sub.loc[valid]\n",
    "    if sub.empty:\n",
    "        daily_feat_vol = pd.DataFrame(np.nan,\n",
    "                                      index=idx_base,\n",
    "                                      columns=[\"volume\", \"turnover\"])\n",
    "    else:\n",
    "        vol_pivot = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "                                    values=\"volume\",   dropna=False)\n",
    "        tur_pivot = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "                                    values=\"turnover\", dropna=False)\n",
    "        # 统一重建权重矩阵\n",
    "        w_vec = (weight_df.set_index(\"bbg\")[\"benchmark_2_weight\"]\n",
    "                 .reindex(vol_pivot.columns))\n",
    "        weight_mat = pd.DataFrame(np.tile(w_vec.values, (len(vol_pivot), 1)),\n",
    "                                  index=vol_pivot.index,\n",
    "                                  columns=vol_pivot.columns)\n",
    "        idx_volume = vol_pivot.apply(\n",
    "            lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        idx_turnov = tur_pivot.apply(\n",
    "            lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        daily_feat_vol = pd.DataFrame(\n",
    "            {\"volume\": idx_volume, \"turnover\": idx_turnov},\n",
    "            index=vol_pivot.index\n",
    "        ).reindex(idx_base)              # 填补缺失日期\n",
    "        \n",
    "    # ========== ②  crowding ==========\n",
    "    sub = (weight_df[[\"bbg\"]]\n",
    "           .merge(crowd_concat_df, on=\"bbg\", how=\"left\")\n",
    "           .sort_values([\"date\", \"bbg\"]))\n",
    "    valid = (sub[\"date\"].notna() &\n",
    "             sub[[\"crowding_factor\", \"crowding_factor_change\",\n",
    "                  \"mom_1m\"]].notna().any(axis=1))\n",
    "    sub = sub.loc[valid]\n",
    "    if sub.empty:\n",
    "        daily_feat_crowd = pd.DataFrame(np.nan,\n",
    "                                        index=idx_base,\n",
    "                                        columns=[\"crowd\", \"crowd_c\", \"mom_1m\"])\n",
    "    else:\n",
    "\n",
    "        cf_pivot  = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "                                            values=\"crowding_factor\", dropna=False)\n",
    "        cfc_pivot = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "                                    values=\"crowding_factor_change\", dropna=False)\n",
    "        mom_pivot = sub.pivot_table(index=\"date\", columns=\"bbg\",\n",
    "                                    values=\"mom_1m\", dropna=False)\n",
    "        w_vec = (weight_df.set_index(\"bbg\")[\"benchmark_2_weight\"]\n",
    "                .reindex(cf_pivot.columns))\n",
    "        weight_mat = pd.DataFrame(np.tile(w_vec.values, (len(cf_pivot), 1)),\n",
    "                                index=cf_pivot.index, columns=cf_pivot.columns)\n",
    "        idx_cf  = cf_pivot.apply(\n",
    "            lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        idx_cfc = cfc_pivot.apply(\n",
    "            lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        idx_mom = mom_pivot.apply(\n",
    "            lambda r: weighted_row_sum(r, weight_mat.loc[r.name]), axis=1)\n",
    "        daily_feat_crowd = pd.DataFrame(\n",
    "            {\"crowd\": idx_cf, \"crowd_c\": idx_cfc, \"mom_1m\": idx_mom},\n",
    "            index=cf_pivot.index\n",
    "        ).reindex(idx_base)\n",
    "    # ========== ③  收益表 ==========\n",
    "    gold_daily_ret = (gold_return\n",
    "                    .pivot_table(index=\"date\",\n",
    "                                values=\"weighted_return_2\",\n",
    "                                aggfunc=\"first\")\n",
    "                    .reindex(idx_base))\n",
    "    # ========== ④  拼 final_df ==========\n",
    "    final_df = (gold_daily_ret\n",
    "                .join(daily_feat_vol, how=\"left\")\n",
    "                .join(daily_feat_crowd, how=\"left\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"date\",\n",
    "                                \"weighted_return_2\": \"ret\"}))\n",
    "    final_df[\"theme\"] = bench_name\n",
    "    return final_df\n",
    "\n",
    "# ----------------------------------------------------\n",
    "parts = []\n",
    "for bench in bench_list:\n",
    "    print(f\"{bench} start\")\n",
    "    df = run_one_benchmark(bench)\n",
    "    if df is not None:\n",
    "        parts.append(df)\n",
    "    #print(f\"{bench} finish\")\n",
    "if not parts:\n",
    "    raise RuntimeError(\"没有拼接出任何结果！\")\n",
    "result_df = pd.concat(parts, ignore_index=True)\n",
    "print(f\":heavy_check_mark: 全部完成，result_df 行数: {len(result_df):,}\")\n",
    "print(result_df.head())\n",
    "result_df.to_csv('return_vol_crowd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=pd.read_csv('return_vol_crowd.csv')\n",
    "result_df=result_df.set_index(result_df.columns[0])\n",
    " # 把第一列设为索引\n",
    "result_df.index.name = None                            # 去掉索引名称\n",
    "#result_df = result_df.drop(columns=result_df.columns[0])\n",
    "result_df['date']=pd.to_datetime(result_df['date'])\n",
    "result_df1 = result_df[~((result_df[\"date\"].dt.weekday >= 5) & (result_df[\"ret\"] == 0))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc83db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0b3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
